{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505dc5c0-bdef-484a-9e6f-b34fdf6d074e",
   "metadata": {},
   "source": [
    "We will seek to investigate if lemmatization can improve the performance of the model using the balanced dataset. As this makes use of a data-augmented dataset, we will preprocess the train and test sets separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74fff41-5b57-412e-9e2f-86d1cfa6df76",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "- [Loading of Libraries](#Loading-of-Libraries) \n",
    "- [Load Datasets](#Load-Datasets)\n",
    "- [Preprocessing: Tokenize and Lemmatize](#Preprocessing:-Tokenize-and-Lemmatize)\n",
    "- [Data Modelling](#Data-Modelling)\n",
    "- [Conclusions and Recommendations](#Conclusions-and-Recommendations)\n",
    "- [Limitations](#Limitations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cc98b-31b4-4d4e-b298-d30226ab3fd1",
   "metadata": {},
   "source": [
    "## Loading of Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c81bec64-0f78-4406-a449-e275b05552b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd       \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer # for lemmatization\n",
    "from nltk.corpus import stopwords # for stopwords removal\n",
    "\n",
    "from sklearn.pipeline import Pipeline # to compactly pack multiple modeling operations\n",
    "from sklearn.naive_bayes import MultinomialNB # to build our classification model\n",
    "\n",
    "# Import TFIDFVectorizer from feature_extraction.text module in sklearn.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score # for model performance assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0896283c-b885-4d31-8bf8-210751b64b50",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8faec58a-ff79-452f-a540-89252a1152d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reply</th>\n",
       "      <th>forum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Originally posted by feb01mel View Post I bet...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worse than 5 years of approx. S$25k each year...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ouch... KrisFlyer online Star Alliance award ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Originally posted by SQJunkie View Post I cal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nothing major as they are moving terminals wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               reply  forum\n",
       "0   Originally posted by feb01mel View Post I bet...      0\n",
       "1   Worse than 5 years of approx. S$25k each year...      1\n",
       "2   Ouch... KrisFlyer online Star Alliance award ...      1\n",
       "3   Originally posted by SQJunkie View Post I cal...      1\n",
       "4   Nothing major as they are moving terminals wi...      0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load datasets\n",
    "test = pd.read_csv('./output/data_augment/test_dataset.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cae1f46d-4d15-4639-b1ac-88ac873a29dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reply</th>\n",
       "      <th>forum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The one from August is there, if you did a se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It really makes me start questioning the abil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SQ HKG-SIN, SQ 871, ECONOMY CLASS. VOML. Dinn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SQ853 CAN-SIN (June 2013) SUPPER (GUANGZHOU T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Looking at last year, my PPS Values per fligh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               reply  forum\n",
       "0   The one from August is there, if you did a se...      0\n",
       "1   It really makes me start questioning the abil...      1\n",
       "2   SQ HKG-SIN, SQ 871, ECONOMY CLASS. VOML. Dinn...      0\n",
       "3   SQ853 CAN-SIN (June 2013) SUPPER (GUANGZHOU T...      0\n",
       "4   Looking at last year, my PPS Values per fligh...      1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_train = pd.read_csv('./output/data_augment/augmented_train_dataset.csv')\n",
    "aug_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0090c3fd-949b-4f35-8807-43c973c06d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reply    16\n",
       "forum     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find null values\n",
    "aug_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6536126e-9b75-4178-b06e-09a57f7efedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null values\n",
    "aug_train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9078d9c-d3a1-4c73-b463-b852e55c4237",
   "metadata": {},
   "source": [
    "## Preprocessing: Tokenize and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ddc1e8d3-d318-4cec-89dc-884393592423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize replies into words\n",
    "test['reply'] = test.apply(lambda row: word_tokenize(row['reply']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9853012a-ccf8-46e6-a343-937e184546ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate lemmatizer.\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "23ee4a80-3b64-4724-a11b-46f83a0255d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize each word in replies\n",
    "test['reply'] = test['reply'].apply(lambda lst: [lemmatizer.lemmatize(word) for word in lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d2a04cb7-69cc-4f56-9a75-2d28d512243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['reply'] = test['reply'].apply(', '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87d895be-e747-4930-817c-2e769d42bad3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reply</th>\n",
       "      <th>forum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Originally, posted, by, feb01mel, View, Post, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Worse, than, 5, year, of, approx, ., S, $, 25k...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ouch, ..., KrisFlyer, online, Star, Alliance, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Originally, posted, by, SQJunkie, View, Post, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nothing, major, a, they, are, moving, terminal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6839</th>\n",
       "      <td>My, letter, wa, also, dated, 14, March, ..., I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6840</th>\n",
       "      <td>Asking, on, behalf, of, a, friend, who, is, PP...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6841</th>\n",
       "      <td>Originally, posted, by, SQflyergirl, View, Pos...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6842</th>\n",
       "      <td>Originally, posted, by, Nick, C, View, Post, A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6843</th>\n",
       "      <td>ahh, nice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6844 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  reply  forum\n",
       "0     Originally, posted, by, feb01mel, View, Post, ...      0\n",
       "1     Worse, than, 5, year, of, approx, ., S, $, 25k...      1\n",
       "2     Ouch, ..., KrisFlyer, online, Star, Alliance, ...      1\n",
       "3     Originally, posted, by, SQJunkie, View, Post, ...      1\n",
       "4     Nothing, major, a, they, are, moving, terminal...      0\n",
       "...                                                 ...    ...\n",
       "6839  My, letter, wa, also, dated, 14, March, ..., I...      1\n",
       "6840  Asking, on, behalf, of, a, friend, who, is, PP...      1\n",
       "6841  Originally, posted, by, SQflyergirl, View, Pos...      1\n",
       "6842  Originally, posted, by, Nick, C, View, Post, A...      1\n",
       "6843                                          ahh, nice      1\n",
       "\n",
       "[6844 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "daf3f482-3e6c-42d1-a82b-a300ddf2247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the same on train dataset\n",
    "# Tokenize replies into words\n",
    "aug_train['reply'] = aug_train.apply(lambda row: word_tokenize(row['reply']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6c1090f8-ff4b-40f2-a001-cd7bcb53a484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize each word in replies\n",
    "aug_train['reply'] = aug_train['reply'].apply(lambda lst: [lemmatizer.lemmatize(word) for word in lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3dfff4ef-6252-4f66-8936-02d8844adcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_train['reply'] = aug_train['reply'].apply(', '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "99c6978a-7f98-4de1-9ba3-6090ef2b26b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reply</th>\n",
       "      <th>forum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The, one, from, August, is, there, ,, if, you,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It, really, make, me, start, questioning, the,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SQ, HKG-SIN, ,, SQ, 871, ,, ECONOMY, CLASS, .,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SQ853, CAN-SIN, (, June, 2013, ), SUPPER, (, G...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Looking, at, last, year, ,, my, PPS, Values, p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23526</th>\n",
       "      <td>Originally, posted, by, kapitan, View, Post, H...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23527</th>\n",
       "      <td>hi, all, ,, have, an, upcoming, flight, on, SQ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23528</th>\n",
       "      <td>I, 'm, missing, the, T2/T3, SKL, agent, alread...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23529</th>\n",
       "      <td>Originally, posted, by, SQueeze, View, Post, W...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23530</th>\n",
       "      <td>Congrats, ,, token01, on, your, PPS, !, Hope, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23515 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   reply  forum\n",
       "0      The, one, from, August, is, there, ,, if, you,...      0\n",
       "1      It, really, make, me, start, questioning, the,...      1\n",
       "2      SQ, HKG-SIN, ,, SQ, 871, ,, ECONOMY, CLASS, .,...      0\n",
       "3      SQ853, CAN-SIN, (, June, 2013, ), SUPPER, (, G...      0\n",
       "4      Looking, at, last, year, ,, my, PPS, Values, p...      1\n",
       "...                                                  ...    ...\n",
       "23526  Originally, posted, by, kapitan, View, Post, H...      1\n",
       "23527  hi, all, ,, have, an, upcoming, flight, on, SQ...      0\n",
       "23528  I, 'm, missing, the, T2/T3, SKL, agent, alread...      0\n",
       "23529  Originally, posted, by, SQueeze, View, Post, W...      0\n",
       "23530  Congrats, ,, token01, on, your, PPS, !, Hope, ...      1\n",
       "\n",
       "[23515 rows x 2 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664677e-4efd-42bb-9b3b-d571a8a7c12e",
   "metadata": {},
   "source": [
    "## Data Modelling\n",
    "- Set data up for modelling\n",
    "- Modelling with baseline classifier for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f2b2358d-a980-4869-8764-86c51351dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data up for modelling\n",
    "X_train = aug_train['reply']\n",
    "y_train = aug_train['forum']\n",
    "X_test = test['reply']\n",
    "y_test = test['forum']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ebad57-307d-4d98-8355-9fe15f3eb831",
   "metadata": {},
   "source": [
    "### Modelling using TfidfVectorizer and baseline classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ee815b36-9916-4232-bdaa-aca9c1861656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a pipeline with tf-idf vectorizer and multinomial naive bayes\n",
    "\n",
    "pipe_tvec = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(stop_words='english')),\n",
    "    ('nb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5d53c2ba-2536-4c7e-91f6-d1d29e61205b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;nb&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;nb&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tvec', TfidfVectorizer(stop_words='english')),\n",
       "                ('nb', MultinomialNB())])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit pipeline to training data\n",
    "pipe_tvec.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "63503b13-a3d2-480a-b8df-465a04197460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.907250691048267"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model on training set\n",
    "pipe_tvec.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "efb56193-357c-4f11-9e41-5de6d3b8e782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8885154880187025"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model on testing set\n",
    "pipe_tvec.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3544e36e-c81e-4781-a09a-4d8d6aa8e15d",
   "metadata": {},
   "source": [
    "When lemmatization was done, the train and test scores did not improve, with the vectorizer and estimator kept constant. Thus, the model with lemmatization will not be optimised.\n",
    "\n",
    "|**Metrics**|**Model without lemmatization**|**Model with lemmatization**|\n",
    "|:---|:---|:---|\n",
    "|train_score|0.909|0.907|\n",
    "|test_score|0.893|0.889|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c0cfa6-86f5-4091-b43c-f5b73d7107a9",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations\n",
    "- The objective of this project is to build a machine learning algorithm for an airline company to direct incoming queries to the right channel, namely the membership programme (Krisflyer) and services (inflight catering, amentities and lounges). \n",
    "\n",
    "1. Firstly, webscraping was done from an online forum dedicated to the airline company since queries were posted there especially when customers could not reach a customer support agent.\n",
    "\n",
    "2. A baseline model using CountVectorizer (transformer) and Multinomial Naive Bayes (estimator) was then fitted and evaluated using the following metrics.\n",
    "|**Metrics**|**MultinomialNB()**|\n",
    "|:---|:---|\n",
    "|train_score|0.858|\n",
    "|test_score|0.844|\n",
    "|sensitivity|0.986|\n",
    "|precision|0.801|\n",
    "|f1 score|0.884|\n",
    "|specificity|0.626|\n",
    "|roc auc|0.806|\n",
    ">A model accuracy of more than 80% is great. Furthermore, as the accuracy score of the train dataset is close to and higher than that of the test dataset, there is no evidence of an overfit. Since the objective is to classify a reply to the correct forum category, the cost of false positives (model predicts a reply to be under 'krisflyer' category, when it is actually not) is the same as the cost of false negatives (model predicts a reply to be under 'amenity_catering_lounges' category, when it is actually not) - a dissatisfied costumer. As such, `accuracy` is a suitable metric. However, due to imbalanced nature of the target variable, the `accuracy` metric is not suitable.\n",
    "> The other metrics including f1 score, specificity and roc/auc score were then optimised.\n",
    "\n",
    "3. Model tuning was done by fitting different vectorizers and classifiers to find the best model yielding the highest metrics through GridSearch. \n",
    "|**Metrics**|**MultinomialNB() (baseline)**|**TfidfVectorizer with LinearSVC (with GridSearch4)**|**Zero Shot Classification**|\n",
    "|:---|:---|:---|:---|\n",
    "|train_score|0.858|0.959|-|\n",
    "|test_score|0.844|0.922|0.570|\n",
    "|sensitivity|0.986|0.924|0.407|\n",
    "|precision|0.801|0.946|0.775|\n",
    "|f1 score|0.884|0.935|0.533|\n",
    "|specificity|0.626|0.919|0.819|\n",
    "|roc auc|0.806|0.922|0.613|\n",
    "> Overall, the model with TfidfVectorizer with LinearSVC (with GridSearch4) stands out across different metrics.\n",
    "\n",
    "4. Data augmentation was done to remedy the problem of the imbalanced dataset so that the `accuracy` metric can be used as well.\n",
    "> Compared to the model with an imbalanced target variable, the model with a balanced dataset has a higher accuracy score, with the vectorizer and estimator kept constant.\n",
    "|**Metrics**|**Model with an imbalanced dataset**|**Model with a balanced dataset**|\n",
    "|:---|:---|:---|\n",
    "|train_score|0.887|0.909|\n",
    "|test_score (accuracy score)|0.873|0.893|\n",
    "\n",
    "5. Again, GridSearch was done to find the best model yielding the highest metrics.\n",
    "> Comparing the best models with TfidfVectorizer and LinearSVC with an imbalanced and balanced dataset, the `accuracy` scores are better with the balanced dataset as summarised below.\n",
    "|**Metrics**|**Model with an imbalanced dataset**|**Model with a balanced dataset**|\n",
    "|:---|:---|:---|\n",
    "|train_score|0.959|0.962|\n",
    "|test_score (accuracy score)|0.922|0.923|\n",
    "\n",
    "6. Lemmatization was done in an attempt to further improve the accuracy score on the balanced dataset but the model performed worse.\n",
    "> Thus, the model with lemmatization was not optimised.\n",
    "|**Metrics**|**Model without lemmatization**|**Model with lemmatization**|\n",
    "|:---|:---|:---|\n",
    "|train_score|0.909|0.907|\n",
    "|test_score|0.893|0.889|\n",
    "\n",
    "- The best model is therefore one with TfidfVectorizer and LinearSVC with a balanced dataset, based on the `accuracy` metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba82f76d-2eee-4629-9d7c-d930b7185699",
   "metadata": {},
   "source": [
    "- Moving forward, the airline company can use this model to classify new queries to the correct channel on the chatbot or to the relevant customer support agent.\n",
    "- The project can also be further extended to a multiclass classification model if the airline company wants to direct the query to a more specific channel.\n",
    "- Sentiment analysis can also be conducted using the text data scraped to help the airline company identify its strengths and weaknesses, so that appropriate intervention steps can be implemented to improve its reputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7bbc99-f282-4ebf-a7e8-2135be643932",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "- As the text data was scraped from a forum mainly contributed by Singapore residents, there is the presence of 'Singlish' in the corpus and not being removed through 'English' stopwords removal. However, this is addressed through GridSearch if 'Singlish' is very much prevalent. \n",
    "- Under the 'Catering' fourm, the posts are mainly informational instead of queries, thus the machine learning algorithm might not be able to classify questions related to catering well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5bd67-9f7f-49ff-a0f8-edc03b25adf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
